{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlrYgn3Eo9zo0wmogsG4U3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aridel7/Study_2/blob/main/DLL_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Рекуррентные сети 2**\n",
        "\n",
        "Задание 1\n",
        "\n",
        "Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9)\n",
        "и задавались следующим образом:\n",
        "\n",
        "x - последовательность цифр\n",
        "\n",
        "y1 = x1, y(i) = x(i) + x(1). Если y(i) >= 10, то y(i) = y(i) - 10\n",
        "\n",
        "Задача:\n",
        "\n",
        "научить модель предсказывать y(i) по x(i)\n",
        "пробовать RNN, LSTM, GRU"
      ],
      "metadata": {
        "id": "G9fg724oTlPG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "t1I4nS-nTjP2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_1 = torch.randint(0,9,(5000,25))\n",
        "X_2 = torch.randint(0,9,(5000,75))\n",
        "X_3 = torch.randint(0,9,(5000,150))"
      ],
      "metadata": {
        "id": "xFAod44sVzZQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Y(X_n):\n",
        "  Y = torch.zeros_like(X_n)\n",
        "  for i, X in enumerate(X_n):\n",
        "    for j, x in enumerate(X):\n",
        "      if j == 0:\n",
        "        Y[i][j] = x\n",
        "      else:\n",
        "        Y[i][j] = x + X[0]\n",
        "        if Y[i][j] >= 10:\n",
        "          Y[i][j] -= 10\n",
        "  return Y"
      ],
      "metadata": {
        "id": "OO4lIcKZY1CW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_1 = get_Y(X_1)\n",
        "Y_2 = get_Y(X_3)\n",
        "Y_3 = get_Y(X_3)"
      ],
      "metadata": {
        "id": "ppgMA-lTXmKz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_1, Y_1, test_size=.2, random_state=13)\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()\n",
        "y_train = y_train.float()\n",
        "y_test = y_test.float()"
      ],
      "metadata": {
        "id": "j5WWT1uMulrt"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(torch.nn.Module):\n",
        "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_hiddens = num_hiddens\n",
        "        self.embedding = torch.nn.Embedding(dictionary_size, embedding_size)\n",
        "        self.hidden = rnnClass(embedding_size, num_hiddens)#, batch_first=True)\n",
        "        self.output = torch.nn.Linear(num_hiddens, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.embedding(X)\n",
        "        _, state = self.hidden(out)\n",
        "        #predictions = self.output(state[0].squeeze())\n",
        "        predictions = self.output(_)\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "y78StW7UZ21y"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = NeuralNetwork(torch.nn.RNN, 10, 64, 128, 10)"
      ],
      "metadata": {
        "id": "bdSDG3ICZH9C"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_1.parameters())"
      ],
      "metadata": {
        "id": "MTj7kz4Lb1Sk"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(100):\n",
        "    train_loss, train_acc, iter_num = .0, .0, .0\n",
        "    start = time.time()\n",
        "\n",
        "    model_1.train()\n",
        "    for i in range(int(len(X_train) / 512)):\n",
        "\n",
        "        batch_x = X_train[i * 512:(i + 1) * 512]\n",
        "        batch_y = y_train[i * 512:(i + 1) * 512].flatten()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        answers = model_1.forward(batch_x.to(torch.int64)).view(-1, 10)\n",
        "        loss = criterion(answers, batch_y.to(torch.int64))\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        batch_acc = (answers.argmax(dim=1) == batch_y)\n",
        "        train_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter_num += 1\n",
        "\n",
        "    print(\n",
        "        f\"Epoch: {ep}, loss: {train_loss:.4f}, acc: \"\n",
        "        f\"{train_acc / iter_num:.4f}\",\n",
        "        end=\" | \"\n",
        "    )\n",
        "\n",
        "    test_loss, test_acc, iter_num = .0, .0, .0\n",
        "    model_1.eval()\n",
        "    for i in range(int(len(X_test) / 100)):\n",
        "\n",
        "        batch_x = X_test[i * 100:(i + 1) * 100]\n",
        "        batch_y = y_test[i * 100:(i + 1) * 100].flatten()\n",
        "\n",
        "        answers = model_1.forward(batch_x.to(torch.int64)).view(-1, 10)\n",
        "        loss = criterion(answers, batch_y.to(torch.int64))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        batch_acc = (answers.argmax(dim=1) == batch_y)\n",
        "        test_acc += batch_acc.sum().item() / batch_acc.shape[0]\n",
        "\n",
        "        iter_num += 1\n",
        "    print(\n",
        "        f\"test loss: {test_loss:.4f}, test acc: {test_acc / iter_num:.4f} | \"\n",
        "        f\"{time.time() - start:.2f} sec.\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjMv8IO4ripI",
        "outputId": "95cbb618-97f1-404d-cb1e-5d4c0ab4c5a8"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 15.6937, acc: 0.1253 | test loss: 22.2920, test acc: 0.1392 | 0.98 sec.\n",
            "Epoch: 1, loss: 15.5639, acc: 0.1330 | test loss: 22.1634, test acc: 0.1496 | 0.96 sec.\n",
            "Epoch: 2, loss: 15.4906, acc: 0.1368 | test loss: 22.0922, test acc: 0.1492 | 0.96 sec.\n",
            "Epoch: 3, loss: 15.4480, acc: 0.1369 | test loss: 22.0531, test acc: 0.1492 | 0.95 sec.\n",
            "Epoch: 4, loss: 15.4214, acc: 0.1369 | test loss: 22.0250, test acc: 0.1504 | 1.28 sec.\n",
            "Epoch: 5, loss: 15.4039, acc: 0.1375 | test loss: 22.0073, test acc: 0.1504 | 1.45 sec.\n",
            "Epoch: 6, loss: 15.3919, acc: 0.1372 | test loss: 21.9978, test acc: 0.1492 | 0.95 sec.\n",
            "Epoch: 7, loss: 15.3831, acc: 0.1373 | test loss: 21.9898, test acc: 0.1494 | 0.94 sec.\n",
            "Epoch: 8, loss: 15.3759, acc: 0.1376 | test loss: 21.9841, test acc: 0.1487 | 0.97 sec.\n",
            "Epoch: 9, loss: 15.3697, acc: 0.1377 | test loss: 21.9804, test acc: 0.1486 | 0.98 sec.\n",
            "Epoch: 10, loss: 15.3644, acc: 0.1379 | test loss: 21.9774, test acc: 0.1479 | 0.94 sec.\n",
            "Epoch: 11, loss: 15.3597, acc: 0.1382 | test loss: 21.9751, test acc: 0.1477 | 0.95 sec.\n",
            "Epoch: 12, loss: 15.3553, acc: 0.1385 | test loss: 21.9736, test acc: 0.1472 | 1.24 sec.\n",
            "Epoch: 13, loss: 15.3512, acc: 0.1386 | test loss: 21.9729, test acc: 0.1467 | 1.47 sec.\n",
            "Epoch: 14, loss: 15.3472, acc: 0.1390 | test loss: 21.9723, test acc: 0.1457 | 1.05 sec.\n",
            "Epoch: 15, loss: 15.3434, acc: 0.1389 | test loss: 21.9721, test acc: 0.1445 | 1.25 sec.\n",
            "Epoch: 16, loss: 15.3397, acc: 0.1394 | test loss: 21.9725, test acc: 0.1436 | 1.51 sec.\n",
            "Epoch: 17, loss: 15.3360, acc: 0.1399 | test loss: 21.9730, test acc: 0.1430 | 0.97 sec.\n",
            "Epoch: 18, loss: 15.3323, acc: 0.1402 | test loss: 21.9738, test acc: 0.1423 | 0.98 sec.\n",
            "Epoch: 19, loss: 15.3286, acc: 0.1407 | test loss: 21.9748, test acc: 0.1416 | 1.00 sec.\n",
            "Epoch: 20, loss: 15.3249, acc: 0.1418 | test loss: 21.9761, test acc: 0.1406 | 1.02 sec.\n",
            "Epoch: 21, loss: 15.3211, acc: 0.1421 | test loss: 21.9775, test acc: 0.1406 | 0.98 sec.\n",
            "Epoch: 22, loss: 15.3174, acc: 0.1427 | test loss: 21.9792, test acc: 0.1400 | 1.00 sec.\n",
            "Epoch: 23, loss: 15.3136, acc: 0.1433 | test loss: 21.9810, test acc: 0.1391 | 0.97 sec.\n",
            "Epoch: 24, loss: 15.3097, acc: 0.1439 | test loss: 21.9829, test acc: 0.1383 | 0.96 sec.\n",
            "Epoch: 25, loss: 15.3058, acc: 0.1445 | test loss: 21.9851, test acc: 0.1385 | 0.96 sec.\n",
            "Epoch: 26, loss: 15.3018, acc: 0.1450 | test loss: 21.9873, test acc: 0.1382 | 0.97 sec.\n",
            "Epoch: 27, loss: 15.2977, acc: 0.1460 | test loss: 21.9898, test acc: 0.1376 | 1.37 sec.\n",
            "Epoch: 28, loss: 15.2935, acc: 0.1464 | test loss: 21.9923, test acc: 0.1366 | 1.36 sec.\n",
            "Epoch: 29, loss: 15.2893, acc: 0.1468 | test loss: 21.9951, test acc: 0.1357 | 0.96 sec.\n",
            "Epoch: 30, loss: 15.2849, acc: 0.1473 | test loss: 21.9979, test acc: 0.1353 | 0.95 sec.\n",
            "Epoch: 31, loss: 15.2805, acc: 0.1477 | test loss: 22.0009, test acc: 0.1356 | 0.96 sec.\n",
            "Epoch: 32, loss: 15.2759, acc: 0.1483 | test loss: 22.0041, test acc: 0.1353 | 0.96 sec.\n",
            "Epoch: 33, loss: 15.2713, acc: 0.1487 | test loss: 22.0074, test acc: 0.1336 | 0.95 sec.\n",
            "Epoch: 34, loss: 15.2665, acc: 0.1495 | test loss: 22.0108, test acc: 0.1335 | 0.94 sec.\n",
            "Epoch: 35, loss: 15.2616, acc: 0.1503 | test loss: 22.0144, test acc: 0.1329 | 0.97 sec.\n",
            "Epoch: 36, loss: 15.2566, acc: 0.1511 | test loss: 22.0182, test acc: 0.1320 | 0.97 sec.\n",
            "Epoch: 37, loss: 15.2515, acc: 0.1518 | test loss: 22.0222, test acc: 0.1326 | 0.98 sec.\n",
            "Epoch: 38, loss: 15.2463, acc: 0.1525 | test loss: 22.0263, test acc: 0.1320 | 0.99 sec.\n",
            "Epoch: 39, loss: 15.2410, acc: 0.1532 | test loss: 22.0306, test acc: 0.1316 | 1.37 sec.\n",
            "Epoch: 40, loss: 15.2356, acc: 0.1538 | test loss: 22.0351, test acc: 0.1303 | 1.34 sec.\n",
            "Epoch: 41, loss: 15.2301, acc: 0.1543 | test loss: 22.0397, test acc: 0.1311 | 0.96 sec.\n",
            "Epoch: 42, loss: 15.2245, acc: 0.1552 | test loss: 22.0445, test acc: 0.1298 | 0.95 sec.\n",
            "Epoch: 43, loss: 15.2187, acc: 0.1556 | test loss: 22.0496, test acc: 0.1300 | 0.96 sec.\n",
            "Epoch: 44, loss: 15.2129, acc: 0.1565 | test loss: 22.0547, test acc: 0.1292 | 0.96 sec.\n",
            "Epoch: 45, loss: 15.2069, acc: 0.1573 | test loss: 22.0601, test acc: 0.1290 | 0.96 sec.\n",
            "Epoch: 46, loss: 15.2009, acc: 0.1577 | test loss: 22.0657, test acc: 0.1288 | 0.95 sec.\n",
            "Epoch: 47, loss: 15.1948, acc: 0.1585 | test loss: 22.0714, test acc: 0.1280 | 0.97 sec.\n",
            "Epoch: 48, loss: 15.1885, acc: 0.1590 | test loss: 22.0774, test acc: 0.1282 | 0.95 sec.\n",
            "Epoch: 49, loss: 15.1822, acc: 0.1600 | test loss: 22.0835, test acc: 0.1275 | 1.38 sec.\n",
            "Epoch: 50, loss: 15.1758, acc: 0.1612 | test loss: 22.0898, test acc: 0.1268 | 1.68 sec.\n",
            "Epoch: 51, loss: 15.1692, acc: 0.1621 | test loss: 22.0963, test acc: 0.1260 | 1.45 sec.\n",
            "Epoch: 52, loss: 15.1626, acc: 0.1628 | test loss: 22.1030, test acc: 0.1257 | 1.13 sec.\n",
            "Epoch: 53, loss: 15.1559, acc: 0.1635 | test loss: 22.1098, test acc: 0.1260 | 0.98 sec.\n",
            "Epoch: 54, loss: 15.1491, acc: 0.1646 | test loss: 22.1169, test acc: 0.1254 | 0.95 sec.\n",
            "Epoch: 55, loss: 15.1422, acc: 0.1656 | test loss: 22.1242, test acc: 0.1246 | 0.96 sec.\n",
            "Epoch: 56, loss: 15.1352, acc: 0.1667 | test loss: 22.1316, test acc: 0.1240 | 0.96 sec.\n",
            "Epoch: 57, loss: 15.1281, acc: 0.1678 | test loss: 22.1393, test acc: 0.1240 | 0.94 sec.\n",
            "Epoch: 58, loss: 15.1210, acc: 0.1685 | test loss: 22.1471, test acc: 0.1236 | 0.97 sec.\n",
            "Epoch: 59, loss: 15.1137, acc: 0.1693 | test loss: 22.1552, test acc: 0.1227 | 0.95 sec.\n",
            "Epoch: 60, loss: 15.1064, acc: 0.1704 | test loss: 22.1635, test acc: 0.1220 | 0.94 sec.\n",
            "Epoch: 61, loss: 15.0990, acc: 0.1714 | test loss: 22.1720, test acc: 0.1220 | 0.97 sec.\n",
            "Epoch: 62, loss: 15.0915, acc: 0.1724 | test loss: 22.1807, test acc: 0.1220 | 1.14 sec.\n",
            "Epoch: 63, loss: 15.0840, acc: 0.1733 | test loss: 22.1896, test acc: 0.1220 | 1.45 sec.\n",
            "Epoch: 64, loss: 15.0763, acc: 0.1740 | test loss: 22.1988, test acc: 0.1226 | 1.13 sec.\n",
            "Epoch: 65, loss: 15.0686, acc: 0.1743 | test loss: 22.2081, test acc: 0.1232 | 0.96 sec.\n",
            "Epoch: 66, loss: 15.0609, acc: 0.1748 | test loss: 22.2178, test acc: 0.1231 | 0.96 sec.\n",
            "Epoch: 67, loss: 15.0531, acc: 0.1757 | test loss: 22.2276, test acc: 0.1227 | 0.96 sec.\n",
            "Epoch: 68, loss: 15.0452, acc: 0.1764 | test loss: 22.2377, test acc: 0.1223 | 0.96 sec.\n",
            "Epoch: 69, loss: 15.0372, acc: 0.1776 | test loss: 22.2480, test acc: 0.1220 | 0.97 sec.\n",
            "Epoch: 70, loss: 15.0292, acc: 0.1785 | test loss: 22.2585, test acc: 0.1218 | 0.95 sec.\n",
            "Epoch: 71, loss: 15.0211, acc: 0.1791 | test loss: 22.2692, test acc: 0.1217 | 0.97 sec.\n",
            "Epoch: 72, loss: 15.0129, acc: 0.1799 | test loss: 22.2802, test acc: 0.1220 | 0.96 sec.\n",
            "Epoch: 73, loss: 15.0047, acc: 0.1807 | test loss: 22.2914, test acc: 0.1218 | 0.97 sec.\n",
            "Epoch: 74, loss: 14.9964, acc: 0.1816 | test loss: 22.3027, test acc: 0.1208 | 1.14 sec.\n",
            "Epoch: 75, loss: 14.9881, acc: 0.1827 | test loss: 22.3143, test acc: 0.1210 | 1.41 sec.\n",
            "Epoch: 76, loss: 14.9797, acc: 0.1837 | test loss: 22.3261, test acc: 0.1206 | 1.10 sec.\n",
            "Epoch: 77, loss: 14.9713, acc: 0.1847 | test loss: 22.3381, test acc: 0.1207 | 0.94 sec.\n",
            "Epoch: 78, loss: 14.9628, acc: 0.1851 | test loss: 22.3502, test acc: 0.1200 | 0.95 sec.\n",
            "Epoch: 79, loss: 14.9543, acc: 0.1859 | test loss: 22.3625, test acc: 0.1198 | 0.96 sec.\n",
            "Epoch: 80, loss: 14.9457, acc: 0.1870 | test loss: 22.3750, test acc: 0.1195 | 0.95 sec.\n",
            "Epoch: 81, loss: 14.9371, acc: 0.1878 | test loss: 22.3877, test acc: 0.1200 | 0.97 sec.\n",
            "Epoch: 82, loss: 14.9284, acc: 0.1888 | test loss: 22.4006, test acc: 0.1202 | 0.98 sec.\n",
            "Epoch: 83, loss: 14.9197, acc: 0.1893 | test loss: 22.4135, test acc: 0.1204 | 0.98 sec.\n",
            "Epoch: 84, loss: 14.9109, acc: 0.1901 | test loss: 22.4267, test acc: 0.1201 | 0.97 sec.\n",
            "Epoch: 85, loss: 14.9021, acc: 0.1905 | test loss: 22.4400, test acc: 0.1202 | 0.97 sec.\n",
            "Epoch: 86, loss: 14.8933, acc: 0.1911 | test loss: 22.4534, test acc: 0.1198 | 1.18 sec.\n",
            "Epoch: 87, loss: 14.8844, acc: 0.1921 | test loss: 22.4670, test acc: 0.1196 | 1.45 sec.\n",
            "Epoch: 88, loss: 14.8755, acc: 0.1930 | test loss: 22.4807, test acc: 0.1193 | 1.10 sec.\n",
            "Epoch: 89, loss: 14.8666, acc: 0.1937 | test loss: 22.4946, test acc: 0.1186 | 0.96 sec.\n",
            "Epoch: 90, loss: 14.8575, acc: 0.1945 | test loss: 22.5086, test acc: 0.1185 | 0.96 sec.\n",
            "Epoch: 91, loss: 14.8485, acc: 0.1955 | test loss: 22.5227, test acc: 0.1178 | 0.97 sec.\n",
            "Epoch: 92, loss: 14.8394, acc: 0.1964 | test loss: 22.5369, test acc: 0.1176 | 0.97 sec.\n",
            "Epoch: 93, loss: 14.8302, acc: 0.1972 | test loss: 22.5513, test acc: 0.1169 | 0.99 sec.\n",
            "Epoch: 94, loss: 14.8210, acc: 0.1981 | test loss: 22.5658, test acc: 0.1171 | 0.96 sec.\n",
            "Epoch: 95, loss: 14.8117, acc: 0.1992 | test loss: 22.5804, test acc: 0.1168 | 0.96 sec.\n",
            "Epoch: 96, loss: 14.8024, acc: 0.2002 | test loss: 22.5951, test acc: 0.1168 | 0.97 sec.\n",
            "Epoch: 97, loss: 14.7930, acc: 0.2011 | test loss: 22.6099, test acc: 0.1168 | 0.95 sec.\n",
            "Epoch: 98, loss: 14.7835, acc: 0.2018 | test loss: 22.6249, test acc: 0.1163 | 1.19 sec.\n",
            "Epoch: 99, loss: 14.7740, acc: 0.2029 | test loss: 22.6399, test acc: 0.1166 | 1.44 sec.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(100):\n",
        "    start = time.time()\n",
        "    train_loss = 0.\n",
        "    train_passed = 0\n",
        "\n",
        "    model_1.train()\n",
        "    for X_b, y_b in data_1:\n",
        "        #X_b, y_b = X_b.cuda(), y_b.cuda()\n",
        "        #X_b, y_b = X_b, y_b\n",
        "        optimizer.zero_grad()\n",
        "        answers = model_1(X_b.to(torch.int64)).view(-1, 10)\n",
        "        loss = criterion(answers, y_b.to(torch.int64).reshape(-1,25))\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_passed += 1\n",
        "\n",
        "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))\n",
        "    #model_1.eval()\n",
        "    #generate_text()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "Tbgg2TDrb3cc",
        "outputId": "50828135-dc57-4dc0-fd8f-a785126e3c0b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-299b9b095976>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3053\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3055\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(preds):\n",
        "    softmaxed = torch.softmax(preds, 0)\n",
        "    probas = torch.distributions.multinomial.Multinomial(1, softmaxed).sample()\n",
        "    return probas.argmax()\n",
        "\n",
        "def generate_text():\n",
        "    start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
        "\n",
        "    generated = ''\n",
        "    sentence = text[start_index: start_index + MAX_LEN]\n",
        "    generated += sentence\n",
        "\n",
        "    for i in range(MAX_LEN):\n",
        "        x_pred = torch.zeros((1, MAX_LEN), dtype=int)\n",
        "        for t, char in enumerate(generated[-MAX_LEN:]):\n",
        "            x_pred[0, t] = CHAR_TO_INDEX[char]\n",
        "\n",
        "        preds = model(x_pred.cuda())[0].cpu()\n",
        "        next_char = INDEX_TO_CHAR[sample(preds)]\n",
        "        generated = generated + next_char\n",
        "\n",
        "    print(generated[:MAX_LEN] + '|' + generated[MAX_LEN:])"
      ],
      "metadata": {
        "id": "_o2Ma4Daam7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6eaMRTEVam0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mytVgXH-amoM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}